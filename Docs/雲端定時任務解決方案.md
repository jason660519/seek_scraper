# 雲端定時任務解決方案

## 方案一：GitHub Actions（免費且簡單）

### 優點
- 完全免費（每月2000分鐘執行時間）
- 無需額外服務器
- 自動化程度高
- 與代碼倉庫集成

### 實施步驟

1. **創建 GitHub Actions 工作流**

```yaml
# .github/workflows/scheduled-scraper.yml
name: 定時抓取任務

on:
  schedule:
    - cron: '0 * * * *'  # 每小時執行一次
  workflow_dispatch:  # 允許手動觸發

jobs:
  scrape-data:
    runs-on: ubuntu-latest
    
    steps:
    - name: 檢出代碼
      uses: actions/checkout@v4
      
    - name: 設置 Python 環境
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: 安裝依賴
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: 執行抓取任務
      env:
        PROXY_SOURCES_API_KEY: ${{ secrets.PROXY_SOURCES_API_KEY }}
        NOTIFICATION_EMAIL: ${{ secrets.NOTIFICATION_EMAIL }}
      run: |
        python proxy_management/core/proxy_automation_scheduler.py --config configs/cloud_config.json
        
    - name: 上傳結果
      uses: actions/upload-artifact@v3
      with:
        name: scraped-data-${{ github.run_number }}
        path: |
          proxy_management/data/proxies/
          proxy_management/exports/
          proxy_management/logs/
        retention-days: 30
        
    - name: 提交結果到倉庫
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add proxy_management/data/
        git add proxy_management/exports/
        git diff --quiet && git diff --staged --quiet || git commit -m "自動更新抓取數據 - $(date)"
        git push
```

2. **創建雲端配置文件**

```json
{
  "cloud_mode": true,
  "scheduler": {
    "enabled": true,
    "tasks": [
      {
        "name": "hourly_proxy_fetch",
        "function": "fetch_proxies",
        "schedule": "hourly",
        "enabled": true,
        "config": {
          "sources": ["proxifly", "freeproxylist"],
          "max_proxies": 1000,
          "validation_enabled": true
        }
      },
      {
        "name": "daily_validation",
        "function": "validate_all_proxies",
        "schedule": "daily",
        "enabled": true,
        "config": {
          "batch_size": 100,
          "timeout": 30
        }
      }
    ]
  },
  "data_storage": {
    "type": "git",
    "commit_changes": true,
    "backup_count": 30
  },
  "notifications": {
    "enabled": true,
    "email": {
      "smtp_server": "smtp.gmail.com",
      "smtp_port": 587,
      "username": "${EMAIL_USERNAME}",
      "password": "${EMAIL_PASSWORD}"
    }
  }
}
```

3. **修改自動化調度器支持雲端模式**

```python
# proxy_management/core/cloud_scheduler.py
import os
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional

class CloudScheduler:
    """雲端定時任務調度器"""
    
    def __init__(self, config_path: str):
        self.config = self.load_config(config_path)
        self.setup_logging()
        
    def load_config(self, config_path: str) -> Dict[str, Any]:
        """加載配置文件"""
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # 環境變量覆蓋
        if os.getenv('CLOUD_MODE'):
            config['cloud_mode'] = True
            
        return config
    
    def setup_logging(self):
        """設置日誌記錄"""
        log_dir = Path('logs')
        log_dir.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / f'cloud_scheduler_{datetime.now().strftime("%Y%m%d")}.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def run_hourly_tasks(self):
        """執行每小時任務"""
        self.logger.info("開始執行每小時任務")
        
        try:
            # 1. 獲取新代理
            self.fetch_new_proxies()
            
            # 2. 驗證代理
            self.validate_recent_proxies()
            
            # 3. 生成報告
            self.generate_hourly_report()
            
            # 4. 清理舊數據
            self.cleanup_old_data()
            
            self.logger.info("每小時任務執行完成")
            
        except Exception as e:
            self.logger.error(f"執行每小時任務失敗: {e}")
            self.send_error_notification(f"每小時任務失敗: {e}")
    
    def fetch_new_proxies(self):
        """獲取新代理"""
        from comprehensive_proxy_manager import ComprehensiveProxyManager
        
        manager = ComprehensiveProxyManager()
        result = manager.fetch_and_validate_batch(
            sources=['proxifly', 'freeproxylist'],
            max_proxies=1000,
            validation_enabled=True
        )
        
        self.logger.info(f"獲取了 {result['total_fetched']} 個代理，其中 {len(result['valid_proxies'])} 個有效")
    
    def validate_recent_proxies(self):
        """驗證最近的代理"""
        from proxy_validator import ProxyValidator
        
        validator = ProxyValidator()
        
        # 獲取最近24小時內添加的代理
        recent_proxies = self.get_recent_proxies(hours=24)
        
        if recent_proxies:
            results = validator.batch_validate(recent_proxies, max_workers=50)
            self.logger.info(f"驗證了 {len(recent_proxies)} 個代理")
    
    def generate_hourly_report(self):
        """生成小時報告"""
        from proxy_lifecycle_manager import ProxyLifecycleManager
        
        manager = ProxyLifecycleManager()
        report = manager.generate_analytics_report(
            time_range='1h',
            include_charts=False
        )
        
        # 保存報告
        report_path = Path('reports/hourly') / f"report_{datetime.now().strftime('%Y%m%d_%H')}.json"
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"生成小時報告: {report_path}")
    
    def cleanup_old_data(self):
        """清理舊數據"""
        import shutil
        from datetime import timedelta
        
        cutoff_date = datetime.now() - timedelta(days=30)
        
        # 清理舊的日誌文件
        log_dir = Path('logs')
        for log_file in log_dir.glob('*.log'):
            if datetime.fromtimestamp(log_file.stat().st_mtime) < cutoff_date:
                log_file.unlink()
                self.logger.info(f"刪除舊日誌文件: {log_file}")
        
        # 清理舊的報告文件
        reports_dir = Path('reports')
        for report_file in reports_dir.glob('**/*.json'):
            if datetime.fromtimestamp(report_file.stat().st_mtime) < cutoff_date:
                report_file.unlink()
                self.logger.info(f"刪除舊報告文件: {report_file}")
    
    def send_error_notification(self, error_message: str):
        """發送錯誤通知"""
        if not self.config.get('notifications', {}).get('enabled'):
            return
        
        # 這裡可以集成郵件、Slack、企業微信等通知方式
        self.logger.error(f"錯誤通知: {error_message}")
    
    def get_recent_proxies(self, hours: int = 24):
        """獲取最近添加的代理"""
        # 從數據文件中讀取最近添加的代理
        # 實際實現需要根據數據存儲格式調整
        return []

# 雲端執行入口點
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--cloud":
        scheduler = CloudScheduler("configs/cloud_config.json")
        scheduler.run_hourly_tasks()
    else:
        print("請使用 --cloud 參數運行雲端模式")
```

## 方案二：PythonAnywhere（簡單託管）

### 優點
- 免費賬號可用
- 支持Python腳本定時執行
- 無需服務器管理

### 實施步驟

1. **註冊 PythonAnywhere 賬號**
2. **上傳代碼和依賴文件**
3. **設置定時任務**

```bash
# 在 PythonAnywhere 的 Bash 控制台中
# 創建虛擬環境
mkvirtualenv --python=/usr/bin/python3.8 proxy-scraper

# 安裝依賴
pip install -r requirements.txt

# 創建定時任務腳本
# 在 PythonAnywhere 的 Tasks 頁面設置每小時執行：
# cd /home/yourusername/seek-job-crawler && /home/yourusername/.virtualenvs/proxy-scraper/bin/python proxy_management/core/proxy_automation_scheduler.py --hourly
```

## 方案三：Google Cloud Functions（無服務器）

### 優點
- 按使用量計費
- 自動擴展
- 無需服務器管理

### 實施步驟

```python
# main.py - Google Cloud Function
import functions_framework
from proxy_management.core.cloud_scheduler import CloudScheduler

@functions_framework.http
def hourly_scraper(request):
    """HTTP Cloud Function for hourly scraping"""
    try:
        scheduler = CloudScheduler("configs/cloud_config.json")
        scheduler.run_hourly_tasks()
        
        return {
            'status': 'success',
            'message': 'Hourly scraping completed',
            'timestamp': datetime.now().isoformat()
        }
    except Exception as e:
        return {
            'status': 'error',
            'message': str(e),
            'timestamp': datetime.now().isoformat()
        }, 500

# 部署命令
gcloud functions deploy hourly-scraper \
  --runtime python311 \
  --trigger-http \
  --allow-unauthenticated \
  --entry-point hourly_scraper
```

然後使用 Google Cloud Scheduler 設置每小時觸發：

```bash
gcloud scheduler jobs create http hourly-scraper-job \
  --schedule="0 * * * *" \
  --uri="https://your-region-your-project.cloudfunctions.net/hourly-scraper" \
  --http-method=GET
```

## 方案比較

| 方案 | 成本 | 複雜度 | 可靠性 | 適用場景 |
|------|------|--------|--------|----------|
| GitHub Actions | 免費 | 低 | 高 | 小型項目，代碼在GitHub |
| PythonAnywhere | 免費/低 | 低 | 中 | 簡單Python腳本 |
| Google Cloud | 低 | 中 | 高 | 需要高可靠性 |
| AWS Lambda | 低 | 中 | 高 | 企業級應用 |

## 推薦方案

對於您的需求，我**強烈推薦使用 GitHub Actions**，因為：

1. **完全免費**：每月2000分鐘執行時間足夠使用
2. **設置簡單**：只需創建一個YAML文件
3. **自動備份**：抓取結果自動提交到GitHub倉庫
4. **通知完善**：支持郵件、Slack等多種通知方式
5. **日誌完整**：詳細的執行日誌便於排查問題

## 實施建議

1. **先在本地測試**：確保代碼在本地能正常運行
2. **逐步遷移**：先設置每天執行，確認穩定後改為每小時
3. **監控執行**：定期檢查GitHub Actions的執行日誌
4. **錯誤處理**：確保代碼有完善的錯誤處理和重試機制
5. **數據備份**：重要數據建議同時備份到多個地方

需要我幫您實施其中某個方案嗎？