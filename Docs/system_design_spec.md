# ğŸ—ï¸ ç³»çµ±æ¶æ§‹è¨­è¨ˆ

æœ¬æ–‡æª”é—¡è¿°äº† SEEK çˆ¬èŸ²å°ˆæ¡ˆçš„è»Ÿé«”æ¶æ§‹ã€é—œéµæ¨¡çµ„è¨­è¨ˆã€éŒ¯èª¤è™•ç†ç­–ç•¥ä»¥åŠæ—¥èªŒç³»çµ±ã€‚

## æ¨¡çµ„åŒ–æ¶æ§‹

ç³»çµ±æ¡ç”¨æ¨¡çµ„åŒ–è¨­è¨ˆï¼Œå°‡çˆ¬èŸ²æµç¨‹åˆ†è§£ç‚ºå¹¾å€‹ç¨ç«‹ä¸”å¯é‡ç”¨çš„çµ„ä»¶ã€‚

```python
# --- å°ˆæ¡ˆé€²å…¥é»: main.py ---
async def main():
    """
    å”èª¿æ•´å€‹çˆ¬å–ã€è§£æå’Œå„²å­˜æµç¨‹ã€‚
    """
    # 1. è¼‰å…¥è¨­å®š
    config = load_config()
    
    # 2. åˆå§‹åŒ–æœå‹™ (è³‡æ–™åº«, æ—¥èªŒ)
    db = DatabaseService(config.db_url)
    logger = CrawlerLogger()
    
    # 3. å¯¦ä¾‹åŒ–çˆ¬èŸ²
    crawler = SeekCrawler(config, logger)
    
    # 4. åŸ·è¡Œæœå°‹
    search_params = get_search_params_from_user()
    raw_pages = await crawler.search_jobs(search_params)
    
    # 5. è§£æè³‡æ–™
    parser = JobParser()
    jobs = [parser.parse_job(page) for page in raw_pages]
    
    # 6. å„²å­˜çµæœ
    await db.save_jobs(jobs)

# --- è³‡æ–™æ¨¡å‹: models.py (ä½¿ç”¨ Pydantic) ---
class JobItem(BaseModel):
    """
    å®šç¾©ç¬¦åˆ data_schema_spec.json è¦ç¯„çš„è·ä½è³‡æ–™æ¨¡å‹ã€‚
    æä¾›è³‡æ–™é©—è­‰ã€åºåˆ—åŒ–å’Œé è¨­å€¼ã€‚
    """
    job_id: str
    title: str
    company: Optional[CompanyInfo] = None
    # ... å…¶ä»–æ¬„ä½

# --- æ ¸å¿ƒçˆ¬èŸ²: scrapers.py ---
class SeekCrawler:
    """
    å°è£èˆ‡ SEEK ç¶²ç«™çš„ç¶²è·¯äº’å‹•ã€‚
    """
    def __init__(self, config: CrawlerConfig, logger: CrawlerLogger):
        self.http_client = httpx.AsyncClient(...) # é…ç½®ä»£ç†ã€æ¨™é ­ã€è¶…æ™‚
        self.logger = logger

    async def search_jobs(self, params: SearchParams) -> List[str]:
        """åŸ·è¡Œæœå°‹ä¸¦è¿”å›åŸå§‹ HTML é é¢åˆ—è¡¨ã€‚"""

    async def fetch_job_details(self, job_url: str) -> str:
        """ç²å–å–®ä¸€è·ä½çš„è©³ç´° HTML é é¢ã€‚"""

# --- è§£æå™¨: parsers.py ---
class JobParser:
    """
    å¾åŸå§‹ HTML ä¸­æå–çµæ§‹åŒ–è³‡æ–™ã€‚
    """
    def parse_job_list(self, html: str) -> List[JobItem]:
        """å¾æœå°‹çµæœé è§£æå‡ºå·¥ä½œåˆ—è¡¨ã€‚"""

    def parse_job_details(self, html: str) -> JobItem:
        """å¾è·ä½è©³æƒ…é è§£æå‡ºå®Œæ•´è³‡è¨Šã€‚"""
```

## ğŸ›¡ï¸ éŒ¯èª¤è™•ç†èˆ‡åçˆ¬èŸ²ç­–ç•¥

å¥å£¯çš„éŒ¯èª¤è™•ç†å’Œåçˆ¬èŸ²æ©Ÿåˆ¶æ˜¯å°ˆæ¡ˆæˆåŠŸçš„é—œéµã€‚

### æ™ºæ…§é‡è©¦æ©Ÿåˆ¶

ä½¿ç”¨ `tenacity` æˆ–è‡ªè¨‚è£é£¾å™¨ä¾†å¯¦ç¾æŒ‡æ•¸é€€é¿é‡è©¦ã€‚

```python
from tenacity import retry, stop_after_attempt, wait_random_exponential

@retry(
    wait=wait_random_exponential(multiplier=1, max=60),
    stop=stop_after_attempt(5)
)
async def fetch_url_with_retry(client: httpx.AsyncClient, url: str):
    """
    å¸¶æœ‰æŒ‡æ•¸é€€é¿é‡è©¦æ©Ÿåˆ¶çš„è«‹æ±‚å‡½å¼ã€‚
    """
    response = await client.get(url)
    response.raise_for_status() # é‡å° 4xx/5xx ç‹€æ…‹ç¢¼æ‹‹å‡ºç•°å¸¸
    return response
```

### çµ±ä¸€éŒ¯èª¤è™•ç†å™¨

å»ºç«‹ä¸€å€‹ä¸­å¤®éŒ¯èª¤è™•ç†é¡åˆ¥ï¼Œç”¨æ–¼åˆ†é¡å’Œè¨˜éŒ„ä¸åŒé¡å‹çš„éŒ¯èª¤ã€‚

```python
class ErrorHandler:
    """
    çµ±ä¸€è™•ç†çˆ¬å–éç¨‹ä¸­çš„é æœŸå’Œéé æœŸéŒ¯èª¤ã€‚
    """
    def __init__(self, logger: CrawlerLogger):
        self.logger = logger

    async def handle(self, error: Exception, context: dict):
        if isinstance(error, httpx.HTTPStatusError):
            await self._handle_http_error(error, context)
        elif isinstance(error, httpx.RequestError):
            await self._handle_network_error(error, context)
        else:
            await self._handle_generic_error(error, context)

    async def _handle_http_error(self, error: httpx.HTTPStatusError, context: dict):
        """è™•ç† HTTP 4xx/5xx éŒ¯èª¤ï¼Œå¦‚é€Ÿç‡é™åˆ¶ã€æ‰¾ä¸åˆ°é é¢ã€‚"""
        if error.response.status_code == 429: # Too Many Requests
            self.logger.warning("Rate limit detected. Implementing cooldown.", extra=context)
            # å¯ä»¥åœ¨æ­¤è™•è§¸ç™¼å…¨åŸŸé™æº«æˆ–ä»£ç†è¼ªæ›
        else:
            self.logger.error(f"HTTP error: {error.response.status_code}", extra=context)

    # ... å…¶ä»–éŒ¯èª¤è™•ç†æ–¹æ³•
```

### åçˆ¬èŸ²ç­–ç•¥

1. **è«‹æ±‚é€Ÿç‡æ§åˆ¶**: åœ¨è«‹æ±‚ä¹‹é–“å¼•å…¥éš¨æ©Ÿå»¶é²ï¼ˆä¾‹å¦‚ï¼Œ1-3ç§’ï¼‰ï¼Œæ¨¡æ“¬äººé¡ç€è¦½è¡Œç‚ºã€‚
2. **User-Agent è¼ªæ›**: ç¶­è­·ä¸€å€‹åŒ…å«æœ€æ–°ç€è¦½å™¨ User-Agent çš„åˆ—è¡¨ï¼Œä¸¦åœ¨æ¯æ¬¡è«‹æ±‚æ™‚éš¨æ©Ÿé¸æ“‡ã€‚
3. **ä»£ç†ä¼ºæœå™¨**: ä½¿ç”¨é«˜å“è³ªçš„ä½å®…æˆ–æ•¸æ“šä¸­å¿ƒä»£ç†æ± ï¼Œä¸¦åœ¨è«‹æ±‚å¤±æ•—æˆ–è¢«å°é–æ™‚è‡ªå‹•è¼ªæ› IPã€‚
4. **TLS/JA3 æŒ‡ç´‹æ¨¡æ“¬**: ä½¿ç”¨ `curl_cffi` ç­‰å·¥å…·ä¾†æ¨¡æ“¬ä¸»æµç€è¦½å™¨çš„ TLS æŒ‡ç´‹ï¼Œä»¥è¦é¿ Cloudflare ç­‰é€²éš WAF çš„åµæ¸¬ã€‚
5. **ç„¡é ­ç€è¦½å™¨**: å°æ–¼éœ€è¦ JavaScript æ¸²æŸ“çš„é é¢ï¼Œä½¿ç”¨ Playwright ä¸¦æ­é…ååµæ¸¬æ’ä»¶ï¼ˆå¦‚ `playwright-stealth`ï¼‰ã€‚

## ğŸ“ˆ æ—¥èªŒç®¡ç†ç³»çµ±

ä½¿ç”¨ `rich` å’Œ Python å…§å»ºçš„ `logging` æ¨¡çµ„ä¾†å»ºç«‹çµæ§‹åŒ–çš„æ—¥èªŒç³»çµ±ã€‚

```python
import logging
from rich.logging import RichHandler

class CrawlerLogger:
    """
    æä¾›çµæ§‹åŒ–ã€è‰²å½©è±å¯Œçš„æ—¥èªŒè¨˜éŒ„åŠŸèƒ½ã€‚
    """
    def __init__(self, name="seek_crawler", level="INFO"):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(level)
        
        # é˜²æ­¢é‡è¤‡æ·»åŠ  handler
        if not self.logger.handlers:
            shell_handler = RichHandler()
            file_handler = logging.FileHandler(f"data/logs/{name}.log")
            
            formatter = logging.Formatter(
                "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            )
            file_handler.setFormatter(formatter)
            
            self.logger.addHandler(shell_handler)
            self.logger.addHandler(file_handler)

    def info(self, message: str, extra: dict = None):
        self.logger.info(message, extra=extra)

    def warning(self, message: str, extra: dict = None):
        self.logger.warning(message, extra=extra)

    def error(self, message: str, exc_info=True, extra: dict = None):
        self.logger.error(message, exc_info=exc_info, extra=extra)
```
