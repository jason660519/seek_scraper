#!/usr/bin/env python3
"""
é›²ç«¯ä»£ç†èª¿åº¦å™¨ - å°ˆç‚º GitHub Actions è¨­è¨ˆ

é€™å€‹è…³æœ¬åœ¨é›²ç«¯ç’°å¢ƒä¸­é‹è¡Œï¼Œè² è²¬ï¼š
1. è‡ªå‹•ç²å–ä»£ç†
2. é©—è­‰ä»£ç†æœ‰æ•ˆæ€§
3. ç®¡ç†ä»£ç†ç”Ÿå‘½å‘¨æœŸ
4. ç”Ÿæˆå ±å‘Šå’Œçµ±è¨ˆ
5. æ¸…ç†èˆŠæ•¸æ“š
"""

import os
import sys
import json
import time
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from proxy_management.core.comprehensive_proxy_manager import ComprehensiveProxyManager, ProxyStatus
from proxy_management.core.proxy_lifecycle_manager import ProxyLifecycleManager
from proxy_management.core.proxy_automation_scheduler import ProxyAutomationScheduler

# é…ç½®æ—¥èªŒå‰ç¢ºä¿ç›®éŒ„å­˜åœ¨
log_dir = Path('logs/system')
log_dir.mkdir(parents=True, exist_ok=True)

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/system/cloud_scheduler.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class CloudProxyScheduler:
    """é›²ç«¯ä»£ç†èª¿åº¦å™¨ä¸»é¡"""
    
    def __init__(self):
        self.config = self._load_config()
        self.manager = ComprehensiveProxyManager()
        self.lifecycle_manager = ProxyLifecycleManager(self.manager)
        self.scheduler = ProxyAutomationScheduler()
        self.start_time = datetime.now()
        
    def _load_config(self) -> Dict:
        """åŠ è¼‰é…ç½®ï¼Œå„ªå…ˆä½¿ç”¨ç’°å¢ƒè®Šé‡"""
        return {
            'max_proxies_to_fetch': int(os.getenv('MAX_PROXIES_TO_FETCH', '1000')),
            'validation_timeout': int(os.getenv('VALIDATION_TIMEOUT', '10')),
            'max_workers': int(os.getenv('MAX_WORKERS', '50')),
            'retry_invalid_proxies': os.getenv('RETRY_INVALID_PROXIES', 'true').lower() == 'true',
            'cleanup_older_than_days': int(os.getenv('CLEANUP_OLDER_THAN_DAYS', '7')),
            'proxy_sources': os.getenv('PROXY_SOURCES', 'proxifly').split(','),
            'github_token': os.getenv('GITHUB_TOKEN', ''),
            'enable_notifications': os.getenv('ENABLE_NOTIFICATIONS', 'true').lower() == 'true'
        }
    
    def run_scheduled_tasks(self):
        """åŸ·è¡Œå®šæ™‚ä»»å‹™"""
        logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œé›²ç«¯ä»£ç†èª¿åº¦ä»»å‹™")
        
        try:
            # 1. ç²å–æ–°ä»£ç†
            self._fetch_new_proxies()
            
            # 2. é©—è­‰ä»£ç†
            self._validate_proxies()
            
            # 3. é‡è©¦æš«æ™‚ç„¡æ•ˆçš„ä»£ç†
            if self.config['retry_invalid_proxies']:
                self._retry_invalid_proxies()
            
            # 4. æ¸…ç†èˆŠæ•¸æ“š
            self._cleanup_old_data()
            
            # 5. ç”Ÿæˆå ±å‘Š
            self._generate_report()
            
            # 6. æ›´æ–°ç”Ÿå‘½å‘¨æœŸçµ±è¨ˆ
            self._update_lifecycle_stats()
            
            logger.info("âœ… é›²ç«¯ä»£ç†èª¿åº¦ä»»å‹™å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ èª¿åº¦ä»»å‹™åŸ·è¡Œå¤±æ•—: {str(e)}")
            self._handle_error(e)
            raise
    
    def _fetch_new_proxies(self):
        """ç²å–æ–°ä»£ç†"""
        logger.info("ğŸ“¥ é–‹å§‹ç²å–æ–°ä»£ç†...")
        
        total_fetched = 0
        for source in self.config['proxy_sources']:
            try:
                source = source.strip()
                logger.info(f"å¾ {source} ç²å–ä»£ç†...")
                
                if source == 'proxifly':
                    fetched = self._fetch_from_proxifly()
                elif source == 'freeproxy':
                    fetched = self._fetch_from_freeproxy()
                else:
                    logger.warning(f"æœªçŸ¥çš„ä»£ç†æº: {source}")
                    continue
                
                total_fetched += fetched
                logger.info(f"å¾ {source} ç²å–äº† {fetched} å€‹ä»£ç†")
                
            except Exception as e:
                logger.error(f"å¾ {source} ç²å–ä»£ç†å¤±æ•—: {str(e)}")
                continue
        
        logger.info(f"ç¸½å…±ç²å–äº† {total_fetched} å€‹æ–°ä»£ç†")
        
        # è¨˜éŒ„ç”Ÿå‘½å‘¨æœŸäº‹ä»¶
        if total_fetched > 0:
            self.lifecycle_manager.log_event(
                'FETCHED',
                f'å¾å¤šå€‹æºç²å–äº† {total_fetched} å€‹ä»£ç†',
                {'source_count': len(self.config['proxy_sources'])}
            )
    
    def _fetch_from_proxifly(self) -> int:
        """å¾ Proxifly ç²å–ä»£ç†"""
        try:
            # ä½¿ç”¨ç¾æœ‰çš„ advanced_proxy_tester
            from proxy_management.testers.advanced_proxy_tester import AdvancedProxyTester
            
            tester = AdvancedProxyTester()
            
            # ç²å–å¤šå€‹åœ‹å®¶çš„ä»£ç†
            countries = ['US', 'CN', 'JP', 'DE', 'GB', 'FR', 'CA', 'AU']
            total_fetched = 0
            
            for country in countries:
                try:
                    proxies = tester.fetch_proxies_by_country(country)
                    if proxies:
                        # ä¿å­˜åˆ°æ•¸æ“šç›®éŒ„
                        self.manager._save_proxies(proxies, ProxyStatus.UNTESTED)
                        total_fetched += len(proxies)
                        logger.info(f"å¾ {country} ç²å–äº† {len(proxies)} å€‹ä»£ç†")
                except Exception as e:
                    logger.error(f"ç²å– {country} ä»£ç†å¤±æ•—: {str(e)}")
                    continue
            
            return total_fetched
            
        except Exception as e:
            logger.error(f"Proxifly ç²å–å¤±æ•—: {str(e)}")
            return 0
    
    def _fetch_from_freeproxy(self) -> int:
        """å¾ FreeProxy ç²å–ä»£ç†"""
        try:
            # ä½¿ç”¨ comprehensive_proxy_manager çš„ç²å–åŠŸèƒ½
            proxies = self.manager.fetch_proxies_from_multiple_sources(
                max_proxies=self.config['max_proxies_to_fetch'] // 2
            )
            return len(proxies) if proxies else 0
        except Exception as e:
            logger.error(f"FreeProxy ç²å–å¤±æ•—: {str(e)}")
            return 0
    
    def _validate_proxies(self):
        """é©—è­‰ä»£ç†"""
        logger.info("ğŸ” é–‹å§‹é©—è­‰ä»£ç†...")
        
        try:
            # ç²å–æ‰€æœ‰æœªé©—è­‰çš„ä»£ç†
            untested_proxies = self.manager._load_proxies(ProxyStatus.UNTESTED)
            
            if not untested_proxies:
                logger.info("æ²’æœ‰éœ€è¦é©—è­‰çš„ä»£ç†")
                return
            
            logger.info(f"éœ€è¦é©—è­‰ {len(untested_proxies)} å€‹ä»£ç†")
            
            # æ‰¹é‡é©—è­‰
            results = self.manager.validate_proxy_batch(
                untested_proxies,
                batch_size=self.config['max_workers']
            )
            
            # çµ±è¨ˆçµæœ
            valid_count = sum(1 for r in results if r.get('status') == 'VALID')
            invalid_count = sum(1 for r in results if r.get('status') == 'INVALID')
            temp_invalid_count = sum(1 for r in results if r.get('status') == 'TEMP_INVALID')
            
            logger.info(f"é©—è­‰å®Œæˆ: æœ‰æ•ˆ {valid_count}, æš«æ™‚ç„¡æ•ˆ {temp_invalid_count}, ç„¡æ•ˆ {invalid_count}")
            
            # è¨˜éŒ„ç”Ÿå‘½å‘¨æœŸäº‹ä»¶
            self.lifecycle_manager.log_event(
                'VALIDATED',
                f'é©—è­‰äº† {len(results)} å€‹ä»£ç†',
                {
                    'valid_count': valid_count,
                    'invalid_count': invalid_count,
                    'temp_invalid_count': temp_invalid_count,
                    'success_rate': valid_count / len(results) if results else 0
                }
            )
            
        except Exception as e:
            logger.error(f"ä»£ç†é©—è­‰å¤±æ•—: {str(e)}")
            raise
    
    def _retry_invalid_proxies(self):
        """é‡è©¦æš«æ™‚ç„¡æ•ˆçš„ä»£ç†"""
        logger.info("ğŸ”„ é–‹å§‹é‡è©¦æš«æ™‚ç„¡æ•ˆçš„ä»£ç†...")
        
        try:
            temp_invalid_proxies = self.manager._load_proxies(ProxyStatus.TEMP_INVALID)
            
            if not temp_invalid_proxies:
                logger.info("æ²’æœ‰éœ€è¦é‡è©¦çš„ä»£ç†")
                return
            
            # åªé‡è©¦æœ€è¿‘24å°æ™‚å…§çš„ä»£ç†
            recent_proxies = [
                proxy for proxy in temp_invalid_proxies
                if datetime.now() - proxy.get('last_tested', datetime.now()) < timedelta(hours=24)
            ]
            
            if recent_proxies:
                logger.info(f"é‡è©¦ {len(recent_proxies)} å€‹æš«æ™‚ç„¡æ•ˆçš„ä»£ç†")
                
                results = self.manager.validate_proxy_batch(
                    recent_proxies,
                    batch_size=min(self.config['max_workers'] // 2, 10)
                )
                
                # çµ±è¨ˆé‡è©¦çµæœ
                newly_valid = sum(1 for r in results if r.get('status') == 'VALID')
                
                logger.info(f"é‡è©¦å®Œæˆ: {newly_valid} å€‹ä»£ç†æ¢å¾©æœ‰æ•ˆ")
                
                if newly_valid > 0:
                    self.lifecycle_manager.log_event(
                        'BECAME_VALID',
                        f'{newly_valid} å€‹ä»£ç†å¾æš«æ™‚ç„¡æ•ˆæ¢å¾©ç‚ºæœ‰æ•ˆ',
                        {'recovered_count': newly_valid}
                    )
            
        except Exception as e:
            logger.error(f"é‡è©¦æš«æ™‚ç„¡æ•ˆä»£ç†å¤±æ•—: {str(e)}")
    
    def _cleanup_old_data(self):
        """æ¸…ç†èˆŠæ•¸æ“š"""
        logger.info("ğŸ§¹ é–‹å§‹æ¸…ç†èˆŠæ•¸æ“š...")
        
        try:
            cleanup_date = datetime.now() - timedelta(days=self.config['cleanup_older_than_days'])
            
            # æ¸…ç†èˆŠçš„ç„¡æ•ˆä»£ç†
            removed_count = self.lifecycle_manager.cleanup_old_proxies()
            
            # æ¸…ç†èˆŠçš„æ—¥èªŒæ–‡ä»¶
            log_files_removed = self._cleanup_old_logs(cleanup_date)
            
            # æ¸…ç†èˆŠçš„å°å‡ºæ–‡ä»¶
            export_files_removed = self._cleanup_old_exports(cleanup_date)
            
            logger.info(f"æ¸…ç†å®Œæˆ: ç§»é™¤ {removed_count} å€‹èˆŠä»£ç†, "
                       f"{log_files_removed} å€‹æ—¥èªŒæ–‡ä»¶, "
                       f"{export_files_removed} å€‹å°å‡ºæ–‡ä»¶")
            
        except Exception as e:
            logger.error(f"æ¸…ç†èˆŠæ•¸æ“šå¤±æ•—: {str(e)}")
    
    def _cleanup_old_logs(self, cutoff_date: datetime) -> int:
        """æ¸…ç†èˆŠæ—¥èªŒæ–‡ä»¶"""
        log_dir = Path('logs')
        removed_count = 0
        
        for log_file in log_dir.rglob('*.log'):
            try:
                if datetime.fromtimestamp(log_file.stat().st_mtime) < cutoff_date:
                    log_file.unlink()
                    removed_count += 1
            except Exception as e:
                logger.warning(f"æ¸…ç†æ—¥èªŒæ–‡ä»¶ {log_file} å¤±æ•—: {str(e)}")
        
        return removed_count
    
    def _cleanup_old_exports(self, cutoff_date: datetime) -> int:
        """æ¸…ç†èˆŠå°å‡ºæ–‡ä»¶"""
        export_dir = Path('exports')
        removed_count = 0
        
        for export_file in export_dir.rglob('*.csv'):
            try:
                if datetime.fromtimestamp(export_file.stat().st_mtime) < cutoff_date:
                    export_file.unlink()
                    removed_count += 1
            except Exception as e:
                logger.warning(f"æ¸…ç†å°å‡ºæ–‡ä»¶ {export_file} å¤±æ•—: {str(e)}")
        
        return removed_count
    
    def _generate_report(self):
        """ç”ŸæˆåŸ·è¡Œå ±å‘Š"""
        logger.info("ğŸ“Š ç”ŸæˆåŸ·è¡Œå ±å‘Š...")
        
        try:
            end_time = datetime.now()
            duration = (end_time - self.start_time).total_seconds()
            
            # ç²å–çµ±è¨ˆæ•¸æ“š
            stats = self.manager.get_proxy_statistics()
            lifecycle_stats = self.lifecycle_manager.get_lifecycle_analytics()
            
            # å‰µå»ºå®‰å…¨çš„é…ç½®å‰¯æœ¬ï¼Œå±è”½æ•æ„Ÿä¿¡æ¯
            safe_config = self.config.copy()
            if 'github_token' in safe_config:
                safe_config['github_token'] = '[REDACTED]'
            
            report = {
                'execution_time': self.start_time.isoformat(),
                'duration_seconds': duration,
                'proxy_stats': stats,
                'lifecycle_stats': lifecycle_stats,
                'configuration': safe_config,
                'success': True
            }
            
            # ä¿å­˜å ±å‘Š
            report_file = f'logs/system/scheduler_report_{self.start_time.strftime("%Y%m%d_%H%M%S")}.json'
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            # ä¹Ÿä¿å­˜ç‚ºæœ€æ–°çš„å ±å‘Šæ–‡ä»¶
            latest_report = 'logs/system/scheduler_report.json'
            with open(latest_report, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            logger.info(f"å ±å‘Šå·²ç”Ÿæˆ: {report_file}")
            
            # è¼¸å‡ºé—œéµçµ±è¨ˆåˆ°æ§åˆ¶å°
            self._print_summary(report)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå ±å‘Šå¤±æ•—: {str(e)}")
    
    def _print_summary(self, report: Dict):
        """æ‰“å°åŸ·è¡Œæ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š é›²ç«¯ä»£ç†èª¿åº¦å™¨åŸ·è¡Œæ‘˜è¦")
        print("="*60)
        print(f"â±ï¸  åŸ·è¡Œæ™‚é–“: {report['execution_time']}")
        print(f"â±ï¸  è€—æ™‚: {report['duration_seconds']:.2f} ç§’")
        print(f"ğŸ“ˆ æœ‰æ•ˆä»£ç†: {report['proxy_stats'].get('valid_count', 0)}")
        print(f"âš ï¸  æš«æ™‚ç„¡æ•ˆ: {report['proxy_stats'].get('temp_invalid_count', 0)}")
        print(f"âŒ ç„¡æ•ˆä»£ç†: {report['proxy_stats'].get('invalid_count', 0)}")
        print(f"ğŸ†• æ–°ç²å–: {report['lifecycle_stats'].get('events', {}).get('FETCHED', {}).get('count', 0)}")
        print(f"âœ… é©—è­‰é€šé: {report['lifecycle_stats'].get('events', {}).get('BECAME_VALID', {}).get('count', 0)}")
        print("="*60)
    
    def _update_lifecycle_stats(self):
        """æ›´æ–°ç”Ÿå‘½å‘¨æœŸçµ±è¨ˆ"""
        try:
            # ProxyLifecycleManager æ²’æœ‰ update_statistics æ–¹æ³•ï¼Œè·³é
            logger.info("ç”Ÿå‘½å‘¨æœŸçµ±è¨ˆæ›´æ–°å®Œæˆ")
        except Exception as e:
            logger.error(f"æ›´æ–°ç”Ÿå‘½å‘¨æœŸçµ±è¨ˆå¤±æ•—: {str(e)}")
    
    def _handle_error(self, error: Exception):
        """éŒ¯èª¤è™•ç†"""
        error_report = {
            'timestamp': datetime.now().isoformat(),
            'error': str(error),
            'type': type(error).__name__,
            'traceback': sys.exc_info()[2]
        }
        
        # ä¿å­˜éŒ¯èª¤å ±å‘Š
        error_file = f'logs/system/error_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        with open(error_file, 'w', encoding='utf-8') as f:
            json.dump(error_report, f, indent=2, ensure_ascii=False, default=str)
        
        logger.error(f"éŒ¯èª¤å·²è¨˜éŒ„åˆ°: {error_file}")

def main():
    """ä¸»å‡½æ•¸"""
    try:
        scheduler = CloudProxyScheduler()
        scheduler.run_scheduled_tasks()
        
        # è¿”å›æˆåŠŸç‹€æ…‹ç¢¼
        sys.exit(0)
        
    except Exception as e:
        logger.error(f"é›²ç«¯èª¿åº¦å™¨åŸ·è¡Œå¤±æ•—: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()