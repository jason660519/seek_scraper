#!/usr/bin/env python3
"""
ä»£ç†IPæœ‰æ•ˆæ€§é©—è­‰å·¥å…·
æ”¯æ´æ‰¹é‡æ¸¬è©¦ä»£ç†çš„é€£æ¥æ€§ã€éŸ¿æ‡‰æ™‚é–“å’ŒåŒ¿åæ€§
"""

import argparse
import pandas as pd
import csv
from datetime import datetime
import logging
from pathlib import Path
import time
import json
from typing import List, Dict, Tuple, Optional
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor
import requests

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('proxy_validator.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ProxyValidator:
    """ä»£ç†IPé©—è­‰å™¨"""
    
    def __init__(self, timeout: int = 10, max_workers: int = 50):
        self.timeout = timeout
        self.max_workers = max_workers
        self.test_urls = [
            'http://httpbin.org/ip',
            'http://icanhazip.com',
            'http://ipinfo.io/ip',
            'http://checkip.amazonaws.com',
        ]
        self.results = []
        
    def load_proxies_from_csv(self, csv_file: str) -> List[Dict]:
        """å¾CSVæª”æ¡ˆè¼‰å…¥ä»£ç†åˆ—è¡¨"""
        proxies = []
        try:
            df = pd.read_csv(csv_file)
            for _, row in df.iterrows():
                if 'ip' in row and 'port' in row:
                    proxy_type = row.get('type', 'http').lower()
                    proxies.append({
                        'ip': row['ip'],
                        'port': int(row['port']),
                        'type': proxy_type,
                        'country': row.get('country', 'Unknown'),
                        'source': csv_file
                    })
        except Exception as e:
            logger.error(f"è¼‰å…¥CSVæª”æ¡ˆå¤±æ•— {csv_file}: {e}")
        
        return proxies
    
    def load_proxies_from_txt(self, txt_file: str, proxy_type: str = 'http') -> List[Dict]:
        """å¾TXTæª”æ¡ˆè¼‰å…¥ä»£ç†åˆ—è¡¨"""
        proxies = []
        try:
            with open(txt_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and ':' in line:
                        # è™•ç†å¯èƒ½åŒ…å«å”è­°å‰ç¶´çš„URL
                        if '://' in line:
                            line = line.split('://', 1)[1]
                        
                        # åˆ†å‰²IPå’Œç«¯å£
                        if ':' in line:
                            ip, port = line.rsplit(':', 1)
                            try:
                                port_num = int(port.strip())
                                proxies.append({
                                    'ip': ip.strip(),
                                    'port': port_num,
                                    'type': proxy_type,
                                    'country': 'Unknown',
                                    'source': txt_file
                                })
                            except ValueError:
                                logger.warning(f"ç„¡æ•ˆçš„ç«¯å£è™Ÿ: {port}")
                                continue
        except Exception as e:
            logger.error(f"è¼‰å…¥TXTæª”æ¡ˆå¤±æ•— {txt_file}: {e}")
        
        return proxies
    
    def test_proxy_sync(self, proxy: Dict) -> Dict:
        """åŒæ­¥æ¸¬è©¦å–®å€‹ä»£ç†"""
        proxy_url = f"{proxy['type']}://{proxy['ip']}:{proxy['port']}"
        
        result = {
            'ip': proxy['ip'],
            'port': proxy['port'],
            'type': proxy['type'],
            'country': proxy['country'],
            'source': proxy['source'],
            'is_working': False,
            'response_time': None,
            'error_message': None,
            'test_time': datetime.now().isoformat(),
            'anonymity_level': 'Unknown'
        }
        
        try:
            proxies = {
                'http': proxy_url,
                'https': proxy_url
            }
            
            start_time = time.time()
            
            # æ¸¬è©¦åŸºæœ¬é€£æ¥æ€§
            response = requests.get(
                self.test_urls[0],
                proxies=proxies,
                timeout=self.timeout,
                headers={'User-Agent': 'ProxyValidator/1.0'}
            )
            
            response_time = round((time.time() - start_time) * 1000, 2)  # ms
            
            if response.status_code == 200:
                result['is_working'] = True
                result['response_time'] = response_time
                
                # æª¢æŸ¥åŒ¿åæ€§
                try:
                    response_data = response.json()
                    proxy_ip = response_data.get('origin', '')
                    
                    # å¦‚æœè¿”å›çš„IPæ˜¯ä»£ç†IPï¼Œèªªæ˜æ˜¯åŒ¿åä»£ç†
                    if proxy['ip'] in proxy_ip:
                        result['anonymity_level'] = 'Transparent'
                    else:
                        result['anonymity_level'] = 'Anonymous'
                        
                except:
                    result['anonymity_level'] = 'Unknown'
                    
                logger.info(f"âœ… ä»£ç†å¯ç”¨: {proxy['ip']}:{proxy['port']} ({response_time}ms)")
            else:
                result['error_message'] = f"HTTP {response.status_code}"
                logger.warning(f"âŒ ä»£ç†ä¸å¯ç”¨: {proxy['ip']}:{proxy['port']} - HTTP {response.status_code}")
                
        except requests.exceptions.Timeout:
            result['error_message'] = 'Timeout'
            logger.warning(f"â±ï¸ ä»£ç†è¶…æ™‚: {proxy['ip']}:{proxy['port']}")
            
        except requests.exceptions.ConnectionError:
            result['error_message'] = 'Connection Error'
            logger.warning(f"ğŸ”Œ é€£æ¥éŒ¯èª¤: {proxy['ip']}:{proxy['port']}")
            
        except Exception as e:
            result['error_message'] = str(e)
            logger.error(f"âŒ æ¸¬è©¦å¤±æ•—: {proxy['ip']}:{proxy['port']} - {e}")
        
        return result
    
    def validate_proxies(self, proxies: List[Dict]) -> List[Dict]:
        """æ‰¹é‡é©—è­‰ä»£ç†"""
        logger.info(f"é–‹å§‹é©—è­‰ {len(proxies)} å€‹ä»£ç†...")
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(self.test_proxy_sync, proxy) for proxy in proxies]
            
            results = []
            for i, future in enumerate(concurrent.futures.as_completed(futures)):
                try:
                    result = future.result()
                    results.append(result)
                    
                    if (i + 1) % 10 == 0:
                        working_count = sum(1 for r in results if r['is_working'])
                        logger.info(f"å·²æ¸¬è©¦ {i + 1}/{len(proxies)} å€‹ä»£ç†ï¼Œæœ‰æ•ˆ: {working_count}")
                        
                except Exception as e:
                    logger.error(f"ä»£ç†æ¸¬è©¦ç•°å¸¸: {e}")
        
        self.results = results
        return results
    
    def save_results(self, results: List[Dict], output_file: str = None) -> str:
        """å„²å­˜é©—è­‰çµæœ"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"data/proxies/proxy_validation_{timestamp}.csv"
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        Path(output_file).parent.mkdir(parents=True, exist_ok=True)
        
        # å„²å­˜ç‚ºCSV
        df = pd.DataFrame(results)
        df.to_csv(output_file, index=False, encoding='utf-8')
        
        # çµ±è¨ˆè³‡è¨Š
        total_count = len(results)
        working_count = sum(1 for r in results if r['is_working'])
        working_rate = (working_count / total_count * 100) if total_count > 0 else 0
        
        logger.info(f"é©—è­‰å®Œæˆï¼")
        logger.info(f"ç¸½ä»£ç†æ•¸: {total_count}")
        logger.info(f"æœ‰æ•ˆä»£ç†: {working_count}")
        logger.info(f"æœ‰æ•ˆç‡: {working_rate:.2f}%")
        logger.info(f"çµæœå·²å„²å­˜è‡³: {output_file}")
        
        return output_file
    
    def get_working_proxies(self) -> List[Dict]:
        """ç²å–æœ‰æ•ˆçš„ä»£ç†åˆ—è¡¨"""
        return [r for r in self.results if r['is_working']]
    
    def get_statistics(self) -> Dict:
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        if not self.results:
            return {}
        
        total_count = len(self.results)
        working_count = sum(1 for r in self.results if r['is_working'])
        
        # æŒ‰åœ‹å®¶çµ±è¨ˆ
        country_stats = {}
        for result in self.results:
            country = result['country']
            if country not in country_stats:
                country_stats[country] = {'total': 0, 'working': 0}
            country_stats[country]['total'] += 1
            if result['is_working']:
                country_stats[country]['working'] += 1
        
        # æŒ‰é¡å‹çµ±è¨ˆ
        type_stats = {}
        for result in self.results:
            proxy_type = result['type']
            if proxy_type not in type_stats:
                type_stats[proxy_type] = {'total': 0, 'working': 0}
            type_stats[proxy_type]['total'] += 1
            if result['is_working']:
                type_stats[proxy_type]['working'] += 1
        
        # éŸ¿æ‡‰æ™‚é–“çµ±è¨ˆ
        working_results = self.get_working_proxies()
        response_times = [r['response_time'] for r in working_results if r['response_time']]
        
        stats = {
            'total_proxies': total_count,
            'working_proxies': working_count,
            'success_rate': round((working_count / total_count * 100), 2) if total_count > 0 else 0,
            'country_stats': country_stats,
            'type_stats': type_stats,
            'response_time_stats': {
                'count': len(response_times),
                'min': min(response_times) if response_times else 0,
                'max': max(response_times) if response_times else 0,
                'avg': round(sum(response_times) / len(response_times), 2) if response_times else 0
            }
        }
        
        return stats

def main():
    parser = argparse.ArgumentParser(description='ä»£ç†IPæœ‰æ•ˆæ€§é©—è­‰å·¥å…·')
    parser.add_argument('--csv-file', help='è¦é©—è­‰çš„CSVä»£ç†æª”æ¡ˆ')
    parser.add_argument('--txt-file', help='è¦é©—è­‰çš„TXTä»£ç†æª”æ¡ˆ')
    parser.add_argument('--proxy-type', default='http', choices=['http', 'https', 'socks4', 'socks5'], 
                       help='ä»£ç†é¡å‹ (åƒ…é©ç”¨æ–¼TXTæª”æ¡ˆ)')
    parser.add_argument('--timeout', type=int, default=10, help='é€£æ¥è¶…æ™‚æ™‚é–“(ç§’)')
    parser.add_argument('--max-workers', type=int, default=50, help='æœ€å¤§ä½µç™¼æ•¸')
    parser.add_argument('--output', help='è¼¸å‡ºæª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--validate-latest', action='store_true', help='é©—è­‰æœ€æ–°çš„ä»£ç†æª”æ¡ˆ')
    parser.add_argument('--show-stats', action='store_true', help='é¡¯ç¤ºè©³ç´°çµ±è¨ˆè³‡è¨Š')
    
    args = parser.parse_args()
    
    validator = ProxyValidator(timeout=args.timeout, max_workers=args.max_workers)
    
    # è¼‰å…¥ä»£ç†
    proxies = []
    
    if args.validate_latest:
        # æ‰¾åˆ°æœ€æ–°çš„ä»£ç†æª”æ¡ˆ
        proxy_dir = Path("data/proxies")
        if proxy_dir.exists():
            csv_files = list(proxy_dir.glob("proxies_*.csv"))
            if csv_files:
                latest_file = max(csv_files, key=lambda x: x.stat().st_mtime)
                logger.info(f"è¼‰å…¥æœ€æ–°ä»£ç†æª”æ¡ˆ: {latest_file}")
                proxies = validator.load_proxies_from_csv(str(latest_file))
    
    elif args.csv_file:
        proxies = validator.load_proxies_from_csv(args.csv_file)
    
    elif args.txt_file:
        proxies = validator.load_proxies_from_txt(args.txt_file, args.proxy_type)
    
    else:
        logger.error("è«‹æŒ‡å®šè¦é©—è­‰çš„ä»£ç†æª”æ¡ˆæˆ–ä½¿ç”¨ --validate-latest åƒæ•¸")
        return
    
    if not proxies:
        logger.error("æ²’æœ‰æ‰¾åˆ°å¯é©—è­‰çš„ä»£ç†")
        return
    
    # é©—è­‰ä»£ç†
    results = validator.validate_proxies(proxies)
    
    # å„²å­˜çµæœ
    output_file = validator.save_results(results, args.output)
    
    # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
    stats = validator.get_statistics()
    
    print("\n" + "="*50)
    print("ğŸ“Š é©—è­‰çµ±è¨ˆè³‡è¨Š")
    print("="*50)
    print(f"ç¸½ä»£ç†æ•¸: {stats['total_proxies']}")
    print(f"æœ‰æ•ˆä»£ç†: {stats['working_proxies']}")
    print(f"æˆåŠŸç‡: {stats['success_rate']}%")
    
    if args.show_stats and stats['response_time_stats']['count'] > 0:
        print(f"\nâ±ï¸ éŸ¿æ‡‰æ™‚é–“çµ±è¨ˆ:")
        print(f"  æœ€å¿«: {stats['response_time_stats']['min']}ms")
        print(f"  æœ€æ…¢: {stats['response_time_stats']['max']}ms")
        print(f"  å¹³å‡: {stats['response_time_stats']['avg']}ms")
    
    if args.show_stats:
        print(f"\nğŸŒ æŒ‰åœ‹å®¶çµ±è¨ˆ:")
        for country, stat in stats['country_stats'].items():
            rate = (stat['working'] / stat['total'] * 100) if stat['total'] > 0 else 0
            print(f"  {country}: {stat['working']}/{stat['total']} ({rate:.1f}%)")
        
        print(f"\nğŸ“¡ æŒ‰é¡å‹çµ±è¨ˆ:")
        for proxy_type, stat in stats['type_stats'].items():
            rate = (stat['working'] / stat['total'] * 100) if stat['total'] > 0 else 0
            print(f"  {proxy_type}: {stat['working']}/{stat['total']} ({rate:.1f}%)")
    
    print(f"\nçµæœå·²å„²å­˜è‡³: {output_file}")

if __name__ == "__main__":
    main()