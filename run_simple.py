#!/usr/bin/env python3
"""
Seekçˆ¬èŸ²ç°¡åŒ–é‹è¡Œè…³æœ¬

å¿«é€Ÿå•Ÿå‹•Seekçˆ¬èŸ²çš„ç°¡åŒ–ç‰ˆæœ¬
"""

import os
import json
import asyncio
from pathlib import Path
from datetime import datetime

# å°‡srcç›®éŒ„æ·»åŠ åˆ°Pythonè·¯å¾‘
import sys
sys.path.append(str(Path(__file__).parent / 'src'))

from src.scrapers.seek_crawler import SeekCrawler
from src.utils.logger import setup_global_logging, get_logger
from src.config import load_config as get_config, create_directories


async def run_simple_crawler():
    """ç°¡åŒ–çš„çˆ¬èŸ²é‹è¡Œå‡½æ•¸"""
    
    # ç²å–é…ç½®
    config = get_config()
    
    # å‰µå»ºå¿…è¦çš„ç›®éŒ„
    os.makedirs('data/raw', exist_ok=True)
    os.makedirs('data/processed', exist_ok=True)
    os.makedirs('logs', exist_ok=True)
    
    # è¨­ç½®æ—¥èªŒ
    setup_global_logging(
        level=config['log_level'],
        log_file=f"{config['log_dir']}/seek_crawler_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    )
    
    logger = get_logger(__name__)
    logger.info("é–‹å§‹Seekçˆ¬èŸ²ç°¡åŒ–ç‰ˆæœ¬...")
    
    try:
        # å‰µå»ºçˆ¬èŸ²
        crawler = SeekCrawler(
            headless=config['headless'],
            output_dir=config['raw_data_dir']
        )
        
        # å•Ÿå‹•çˆ¬èŸ²
        await crawler.start()
        logger.info("çˆ¬èŸ²å·²å•Ÿå‹•")
        
        # åŸ·è¡Œçˆ¬èŸ²
        stats = await crawler.crawl_jobs(
            keywords=config['default_keywords'][:3],  # é™åˆ¶é—œéµè©æ•¸é‡
            locations=config['default_locations'][:2],  # é™åˆ¶åœ°é»æ•¸é‡
            max_pages=2  # é™åˆ¶é æ•¸ç”¨æ–¼å¿«é€Ÿæ¸¬è©¦
        )
        
        logger.info(f"çˆ¬èŸ²çµ±è¨ˆ: {stats}")
        
        # ç°¡å–®çš„æ•¸æ“šè§£æ
        raw_data_dir = Path(config['raw_data_dir'])
        processed_data_dir = Path(config['processed_data_dir'])
        processed_data_dir.mkdir(parents=True, exist_ok=True)
        
        jobs_data = []
        
        # éæ­·æ‰€æœ‰åŸå§‹æ•¸æ“šæ–‡ä»¶å¤¾
        for job_folder in raw_data_dir.iterdir():
            if not job_folder.is_dir():
                continue
                
            metadata_file = job_folder / "metadata.json"
            if not metadata_file.exists():
                continue
            
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                job_info = {
                    'title': metadata.get('title', ''),
                    'company': metadata.get('company', ''),
                    'location': metadata.get('location', ''),
                    'url': metadata.get('url', ''),
                    'scraped_at': metadata.get('scraped_at', ''),
                    'folder': job_folder.name
                }
                
                jobs_data.append(job_info)
                
            except Exception as e:
                logger.error(f"è§£ææ–‡ä»¶å¤¾ {job_folder} å¤±æ•—: {e}")
                continue
        
        # ä¿å­˜çµæœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜JSON
        json_file = processed_data_dir / f"seek_jobs_simple_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(jobs_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"çµæœå·²ä¿å­˜åˆ°: {json_file}")
        logger.info(f"å…±è™•ç† {len(jobs_data)} å€‹è·ä½")
        
        return {
            'total_jobs': len(jobs_data),
            'json_file': str(json_file),
            'raw_data_dir': str(raw_data_dir),
            'processed_data_dir': str(processed_data_dir)
        }
        
    except Exception as e:
        logger.error(f"çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {e}")
        raise
        
    finally:
        # æ¸…ç†è³‡æº
        if 'crawler' in locals():
            await crawler.stop()
        logger.info("çˆ¬èŸ²å·²åœæ­¢")


def main():
    """ä¸»å‡½æ•¸"""
    print("Seekçˆ¬èŸ²ç°¡åŒ–ç‰ˆæœ¬")
    print("=" * 50)
    
    try:
        result = asyncio.run(run_simple_crawler())
        
        print(f"\nâœ… çˆ¬èŸ²åŸ·è¡Œå®Œæˆï¼")
        print(f"ğŸ“Š çµ±è¨ˆä¿¡æ¯:")
        print(f"   - ç¸½è·ä½æ•¸: {result['total_jobs']}")
        print(f"   - åŸå§‹æ•¸æ“šç›®éŒ„: {result['raw_data_dir']}")
        print(f"   - è™•ç†å¾Œæ•¸æ“šæ–‡ä»¶: {result['json_file']}")
        
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ¶ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œå¤±æ•—: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)